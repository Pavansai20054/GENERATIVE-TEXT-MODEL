{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from models.gpt_generator import GPTGenerator\n",
    "from models.lstm_generator import LSTMModel, TextTokenizer\n",
    "from utils.preprocess import clean_text, create_sequences\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Environment Verification\n",
    "# ----------------------------\n",
    "print(\"=\"*50)\n",
    "print(\"GPU/CPU Verification\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PyTorch verification\n",
    "torch_available = torch.cuda.is_available()\n",
    "torch_device = torch.device('cuda' if torch_available else 'cpu')\n",
    "print(f\"\\nPyTorch:\")\n",
    "print(f\"• CUDA Available: {torch_available}\")\n",
    "if torch_available:\n",
    "    print(f\"• Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"• CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# TensorFlow verification\n",
    "tf_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nTensorFlow:\")\n",
    "print(f\"• GPUs Available: {len(tf_devices)}\")\n",
    "if tf_devices:\n",
    "    print(f\"• GPU Details: {tf_devices}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. GPT-2 Text Generation\n",
    "# ----------------------------\n",
    "print(\"=\"*50)\n",
    "print(\"GPT-2 Text Generation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize generator\n",
    "gpt = GPTGenerator()\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Artificial intelligence will\"\n",
    "start_time = time.time()\n",
    "generated_text = gpt.generate_text(\n",
    "    prompt,\n",
    "    max_length=150,\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")\n",
    "gpt_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "print(f\"Generation Time: {gpt_time:.2f} seconds\")\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(\"-\"*50)\n",
    "print(generated_text)\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. LSTM Text Generation\n",
    "# ----------------------------\n",
    "print(\"=\"*50)\n",
    "print(\"LSTM Text Generation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load and preprocess data\n",
    "with open('data/sample_texts.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "tokenizer = TextTokenizer(cleaned_text)\n",
    "\n",
    "# Prepare training data\n",
    "sequences, next_chars = create_sequences(cleaned_text, seq_length=50)\n",
    "X = np.array([[tokenizer.char_to_idx[c] for c in seq] for seq in sequences])\n",
    "y = tf.keras.utils.to_categorical(\n",
    "    [tokenizer.char_to_idx[c] for c in next_chars],\n",
    "    num_classes=tokenizer.vocab_size\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "lstm = LSTMModel(tokenizer, seq_length=50)\n",
    "\n",
    "print(\"\\nTraining LSTM model (3 epochs demo)...\")\n",
    "train_start = time.time()\n",
    "lstm.train(X, y, epochs=3, batch_size=128)\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "# Generate text\n",
    "seed = \"The future of AI is\"\n",
    "gen_start = time.time()\n",
    "lstm_output = lstm.generate_text(\n",
    "    seed,\n",
    "    length=200,\n",
    "    temperature=0.6\n",
    ")\n",
    "gen_time = time.time() - gen_start\n",
    "\n",
    "print(f\"\\nTraining Time: {train_time:.2f} seconds\")\n",
    "print(f\"Seed Text: '{seed}'\")\n",
    "print(f\"Generation Time: {gen_time:.2f} seconds\")\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(\"-\"*50)\n",
    "print(lstm_output)\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Performance Comparison\n",
    "# ----------------------------\n",
    "print(\"=\"*50)\n",
    "print(\"Performance Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"GPT-2 Generation Time: {gpt_time:.2f}s\")\n",
    "print(f\"LSTM Training Time (3 epochs): {train_time:.2f}s\")\n",
    "print(f\"LSTM Generation Time: {gen_time:.2f}s\")\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\nGPU memory cleared. Execution complete!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
